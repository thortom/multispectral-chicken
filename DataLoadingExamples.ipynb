{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# # Interactive plots\n",
    "# %matplotlib notebook\n",
    "\n",
    "# Load all packages used\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mypackage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The manualy labeled real dataset from day 2 at Tomra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the real dataset\n",
    "X, Y, info = mypackage.Dataset.load(\"data/tomra\", only_with_contaminant=True)\n",
    "X_train, X_test, Y_train, Y_test = mypackage.Dataset.train_test_split(X, Y, testRatio=0.3)\n",
    "\n",
    "X_test, X_train = mypackage.Dataset.scale(X_test, X_train, scale='GlobalStandardization')\n",
    "# X_test, X_train = mypackage.Dataset.PCA(X_test, X_train, n_components=3, plot=True, whiten=True)\n",
    "\n",
    "print(np.unique(Y_test), Y_test.shape, (X_test.min(), X_test.max()), X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then the fake dataset, which, is by default a absorbance dataset.\n",
    "In order to use it as a pre-taining dataset for the TOMRA dataset, then either one of them need to converterd to absorbance or reflectance like the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the wavelengths to be the same as the real dataset collected at TOMRA\n",
    "wavelengths = mypackage.Dataset.TOMRA_WAVELENGTHS\n",
    "\n",
    "# The fake dataset is by default a absorbance dataset\n",
    "fake_data = mypackage.FakeDataset(wavelengths=wavelengths)\n",
    "X, Y = fake_data.get_images(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching the real dataset from day 2 at Tomra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the labeled data and train on it\n",
    "X_orig, Y_orig, info = mypackage.Dataset.load(\"data/tomra\", only_with_contaminant=False)\n",
    "\n",
    "X, Y = mypackage.Dataset.make_zoomed_in_dataset(X_orig, Y_orig, size=32, sample_multiplication=7, contaminant_type=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the rest of the images from day 2 at Tomra and enrich them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary overwrites the previous key-entery\n",
    "image_centers = {'chicken_01': (27, 58), 'chicken_01': (77, 70), 'chicken_02': (22, 42),\n",
    "                 'chicken_02': (75, 51), 'chicken_03': (28, 45), 'chicken_03': (75, 72),\n",
    "                 'chicken_04': (22, 63), 'chicken_04': (70, 79), 'chicken_05': (19, 90),\n",
    "                 'chicken_05': (80, 90), 'fillet_FM_01': (38, 51), 'fillet_FM_02': (57, 60),\n",
    "                 'fillet_FM_03': (50, 33), 'fillet_FM_05': (34, 66), 'fillet_FM_06': (31, 68),\n",
    "                 'fillet_FM_07': (39, 60), 'fillet_FM_08': (37, 64), 'fillet_FM_09': (34, 50),\n",
    "                 'fillet_FM_10': (37, 54), 'fillet_FM_11': (48, 34), 'fillet_FM_12': (37, 55),\n",
    "                 'fillet_v2_FM_01': (44, 60), 'fillet_v2_FM_02': (44, 51), 'fillet_v2_FM_03': (26, 46),\n",
    "                 'fillet_v2_FM_04': (41, 55), 'fillet_v2_FM_05': (34, 70), 'fillet_v2_FM_06': (34, 56),\n",
    "                 'fillet_v2_FM_07': (31, 65), 'fillet_v2_FM_08': (44, 56), 'fillet_v2_FM_09': (33, 59),\n",
    "                 'fillet_v2_FM_10': (44, 71), 'fillet_v2_FM_11': (45, 55), 'FM_02': (57, 60),\n",
    "                 'FM_03': (60, 37), 'FM_04': (72, 54), 'FM_05': (52, 46), 'FM_06': (52, 49),\n",
    "                 'FM_07': (61, 49), 'FM_08': (42, 55), 'FM_09': (54, 50), 'FM_10': (56, 39),\n",
    "                 'leg_meat_FM_01': (43, 52), 'leg_meat_FM_02': (36, 72), 'leg_meat_FM_03': (64, 55),\n",
    "                 'leg_meat_FM_04': (56, 54), 'leg_meat_FM_05': (36, 51), 'leg_meat_FM_06': (43, 59),\n",
    "                 'leg_meat_FM_07': (48, 54), 'leg_meat_FM_08': (53, 81), 'leg_meat_FM_09': (61, 41),\n",
    "                 'leg_meat_FM_10': (48, 60), 'leg_meat_FM_11': (53, 55), 'leg_meatside_01': (39, 82),\n",
    "                 'leg_meatside_02': (44, 69), 'leg_meatside_03': (38, 54), 'leg_meatside_04': (58, 51),\n",
    "                 'leg_skin_FM_01': (44, 62), 'leg_skin_FM_02': (44, 55), 'leg_skin_FM_03': (32, 56),\n",
    "                 'leg_skin_FM_04': (39, 63), 'leg_skin_FM_05': (45, 52), 'leg_skin_FM_06': (56, 58),\n",
    "                 'leg_skin_FM_07': (44, 67), 'leg_skin_FM_08': (46, 55), 'leg_skin_FM_09': (49, 44),\n",
    "                 'leg_skin_FM_10': (52, 76), 'leg_skin_FM_11': (50, 50), 'leg_skinside_01': (45, 62),\n",
    "                 'leg_skinside_04': (45, 53), 'leg_skinside_05': (56, 45), 'leg_skinside_12': (52, 56),\n",
    "                 'leg_skinside_13': (45, 44), 'leg_skinside_22': (59, 64), 'leg_skinside_23': (58, 78)}\n",
    "file_list = list(image_centers.keys())\n",
    "dataset_folder = \"data/tomra_day1\"\n",
    "X = mypackage.Dataset.load_files(file_list, dataset_folder, with_labels=False)\n",
    "_, X, _ = mypackage.Dataset.scale(None, X, scaler=scaler)\n",
    "\n",
    "size = 32\n",
    "count, _, _, k = X.shape\n",
    "X_test = np.zeros((count, size, size, k))\n",
    "for i, file in enumerate(file_list):\n",
    "    x, y = image_centers[file]\n",
    "    X_test[i] = mypackage.Dataset.zoom_in_on_center(X[i], x, y, size=32)\n",
    "\n",
    "print((X_test.min(), X_test.max()), X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scaling and test train splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = mypackage.Dataset.train_test_split(X, Y, testRatio=0.3)\n",
    "\n",
    "X_test, X_train = mypackage.Dataset.scale(X_test, X_train, scale='GlobalStandardization')\n",
    "# X_test, X_train = mypackage.Dataset.PCA(X_test, X_train, n_components=3, plot=True, whiten=True)\n",
    "\n",
    "print(np.unique(Y_test), Y_test.shape, (X_test.min(), X_test.max()), X_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
